import requests
from bs4 import BeautifulSoup
import pandas as pd

# The URL from which we'll be scraping data
url = "https://learn.microsoft.com/en-us/azure/role-based-access-control/permissions/networking"

# Make a request to the website
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all table elements
tables = soup.findAll('table')

# List to hold all the tables as DataFrame objects
dfs = []

# Loop through all found tables
for i, table in enumerate(tables):
    # Convert table to DataFrame
    df = pd.read_html(str(table))[0]
    dfs.append(df)
    # Save the table as a CSV file (optional)
    df.to_csv(f'table_{i+1}.csv', index=False)

# You can access individual tables from the 'dfs' list
# For example, to print the first table:
if dfs:
    print(dfs[0])
else:
    print("No tables found.")




import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://learn.microsoft.com/en-us/azure/role-based-access-control/permissions/networking"

# Try to make a request to the website
try:
    # Increase the timeout if necessary. The default is usually around 10 seconds.
    # Here, 'timeout' is set to 30 seconds for both the connection and the read timeouts.
    response = requests.get(url, timeout=(30, 30))
    
    # Proceed only if we got a successful response
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        tables = soup.findAll('table')
        dfs = []

        for i, table in enumerate(tables):
            df = pd.read_html(str(table))[0]
            dfs.append(df)
            df.to_csv(f'table_{i+1}.csv', index=False)

        if dfs:
            print(dfs[0])
        else:
            print("No tables found.")
    else:
        print(f"Failed to retrieve the webpage. Status code: {response.status_code}")

# Catch the connection timeout error
except requests.exceptions.Timeout:
    print("The request timed out. Please try again later or check your internet connection.")

# Catch other potential errors
except Exception as e:
    print(f"An error occurred: {e}")



import pandas as pd
import os

# Specify the path to your folder containing the CSV files
folder_path = 'path/to/your/csv/folder'

# Initialize two empty DataFrames, one for each condition
df_with_write = pd.DataFrame()
df_without_write = pd.DataFrame()

# Loop through all files in the specified folder
for filename in os.listdir(folder_path):
    if filename.endswith('.csv'):  # Check if the file is a CSV
        file_path = os.path.join(folder_path, filename)
        # Read the CSV file
        df = pd.read_csv(file_path)
        
        # Check if the first column of the current DataFrame contains 'write'
        if 'write' in df.columns[0].lower():
            # If it does, append it to the 'df_with_write' DataFrame
            df_with_write = df_with_write.append(df, ignore_index=True)
        else:
            # If it does not, append it to the 'df_without_write' DataFrame
            df_without_write = df_without_write.append(df, ignore_index=True)

# At this point, 'df_with_write' contains all rows from all CSVs where the first column had 'write',
# and 'df_without_write' contains all other rows.

# If you want to inspect the DataFrames, you can print them or their first few rows using:
print(df_with_write.head())
print(df_without_write.head())

# If needed, you can also save these DataFrames to new CSV files:
df_with_write.to_csv('with_write.csv', index=False)
df_without_write.to_csv('without_write.csv', index=False)
